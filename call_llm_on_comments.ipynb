{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97221ff",
   "metadata": {},
   "source": [
    "This notebook calls an llm to label youtube comments using externally defined prompts.\n",
    "\n",
    "Currently runs on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5de2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a4cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments(args=None):\n",
    "    \"\"\"Parse command-line arguments or use provided args.\"\"\"\n",
    "    import argparse\n",
    "    import importlib.util\n",
    "    import json\n",
    "\n",
    "    if args is None:\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\"--inp-col\", type=str, required=True, help=\"Input column name in the DataFrame\")\n",
    "        parser.add_argument(\"--out-col\", type=str, required=True, help=\"Output column name for LLM responses\")\n",
    "        parser.add_argument(\"--prompt-name\", type=str, required=True, help=\"Name of the prompt to use\")\n",
    "        parser.add_argument(\"--llm-name\", type=str, default=\"gpt-4o-mini\", help=\"Name of the LLM to use\")\n",
    "        parser.add_argument(\"--debug-mode\", action=\"store_true\", help=\"Run in debug mode with sequential processing\")\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "\n",
    "    # Read API key if not already set\n",
    "    if not hasattr(args, \"api_key\"):\n",
    "        args.api_key = read_api_key()\n",
    "\n",
    "    # Load config and merge into args\n",
    "    config_path = \"configs/constants_labeling.py\"\n",
    "    spec = importlib.util.spec_from_file_location(\"constants_labeling\", config_path)\n",
    "    config = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(config)\n",
    "\n",
    "    # Set attributes from config if not already set by args\n",
    "    for attr in dir(config):\n",
    "        if not attr.startswith(\"__\"):\n",
    "            if not hasattr(args, attr):\n",
    "                setattr(args, attr, getattr(config, attr))\n",
    "\n",
    "    # Load prompt template if not already set\n",
    "    if not hasattr(args, \"prompt_template\"):\n",
    "        args.prompt_path = getattr(args, \"PROMPT_PATH\", None)\n",
    "        with open(args.prompt_path, \"r\") as f:\n",
    "            PROMPT_TEMPLATES = json.load(f)\n",
    "        args.prompt_template = PROMPT_TEMPLATES[args.prompt_name]\n",
    "\n",
    "    # Set endpoint and API version if not already set\n",
    "    if not hasattr(args, \"endpoint\"):\n",
    "        args.endpoint = getattr(args, \"SANDBOX_ENDPOINT\", None)\n",
    "    if not hasattr(args, \"api_version\"):\n",
    "        args.api_version = getattr(args, \"SANDBOX_API_VERSION\", None)\n",
    "\n",
    "    return args\n",
    "\n",
    "def read_api_key(api_key_path=\"openai_api_key.txt\"):\n",
    "    with open(api_key_path, \"r\") as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "def _worker(args_tuple):\n",
    "    \"\"\"Worker function for multiprocessing.\"\"\"\n",
    "    sentence, prompt_template, model_to_be_used, api_key, endpoint, api_version, prompt_name = args_tuple\n",
    "    try:\n",
    "        from openai import AzureOpenAI\n",
    "        client = AzureOpenAI(api_key=api_key, azure_endpoint=endpoint, api_version=api_version)\n",
    "        prompt = [\n",
    "            {\"role\": \"system\", \"content\": \"You are helping in scientific analysis, so please be precise.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_template.format(sentence=sentence)}\n",
    "        ]\n",
    "        # Adjust max_tokens based on prompt_name and model_to_be_used\n",
    "        if \"eval\" in prompt_name and \"Mistral\" in model_to_be_used:\n",
    "            max_tokens = 5\n",
    "        elif \"eval\" in prompt_name:\n",
    "            max_tokens = 1\n",
    "        else:\n",
    "            max_tokens = 1000\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_to_be_used,\n",
    "            temperature=0.0,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1.0,  # Ensure top_p is 1.0 for greedy sampling\n",
    "            messages=prompt\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        # print exception and return empty string\n",
    "        # print(f\"Error processing sentence '{sentence}': {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def call_llm_on_sentences(sentences, args, batch_size=32):\n",
    "    \"\"\"Call LLM on sentences and return results.\"\"\"\n",
    "    args_list = [\n",
    "        (sent, args.prompt_template, args.llm_name, args.api_key, args.endpoint, args.api_version, args.prompt_name)\n",
    "        for sent in sentences\n",
    "    ]\n",
    "    if args.debug_mode:\n",
    "        # Sequential processing for debug mode\n",
    "        results = []\n",
    "        for arg in args_list:\n",
    "            tmp_res = _worker(arg)\n",
    "            results.append(tmp_res)\n",
    "    else:\n",
    "        # Parallel processing\n",
    "        with Pool(processes=min(cpu_count(), batch_size)) as pool:\n",
    "            results = pool.map(_worker, args_list)\n",
    "    return results\n",
    "\n",
    "def process_comments_with_llm(comments_df, args, batch_size=32):\n",
    "    \"\"\"\n",
    "    Process comments in comments_df using an LLM and store the responses in a new column.\n",
    "\n",
    "    Args:\n",
    "        comments_df (pd.DataFrame): DataFrame containing comments to process.\n",
    "        args: Arguments required for the LLM call.\n",
    "        batch_size (int): Number of processes to run in parallel.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional column for LLM responses.\n",
    "    \"\"\"\n",
    "    # Extract the comments as a list\n",
    "    sentences = comments_df['Comments'].tolist()\n",
    "\n",
    "    # Call the LLM on the sentences\n",
    "    responses = call_llm_on_sentences(sentences, args, batch_size=batch_size)\n",
    "\n",
    "    # Add the responses as a new column in the DataFrame\n",
    "    comments_df[args.out_col] = responses\n",
    "\n",
    "    return comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b800e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.read_excel('data/Comments_Dataset.xlsx')\n",
    "comments_df.dropna(subset=['Comments'], inplace=True)\n",
    "\n",
    "# # iterate through df based on 'Song Name'\n",
    "# for song_name, group in comments_df.groupby('Song Name'):\n",
    "#     print(f\"Processing comments for song: {song_name}\")\n",
    "#     print(group.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3733d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# Manually define the arguments\n",
    "args = argparse.Namespace(\n",
    "    inp_col=\"Comments\",\n",
    "    prompt_name=\"eval_music_rel\",\n",
    "    llm_name=\"gpt-4o-mini\",\n",
    "    debug_mode=True\n",
    ")\n",
    "args.out_col=f\"{args.prompt_name}_{args.llm_name}\"\n",
    "args = parse_arguments(args)\n",
    "\n",
    "comments_df = process_comments_with_llm(comments_df, args, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756719df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-sent-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
